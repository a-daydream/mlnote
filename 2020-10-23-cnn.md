# Convolutional Neural network

## Why CNN for image?

### CNN vs DNN

我们可以用一般的neural network做图像处理，会遇到问题，**我们直接用一般的fully connected和feedforward network会需要太多的参数**。

![image-20201023204126225](C:\Users\liucheng\AppData\Roaming\Typora\typora-user-images\image-20201023204126225.png)

比如一张100*100的图片，它的vector的size是 100 * 100 * 3,也就是说input vector就是3万维，假设hidden layer有1000个neuron，那么仅仅第一层就有30000 * 1000 个参数。

所以CNN做的事情是**简化neural network的架构，根据我们自己的知识将一些实际上用不到的参数过滤掉，不用fully connected network，而是用比较少的参数实现图像处理**，所以CNN比DNN更加简单。

### Three Property for CNN theory base

怎么过滤参数呢？有一下三个对于图像处理的观察：（**CNN架构提出的基础**）

> Some patterns are much smaller than the whole image

假设第一层hidden layer里，neuron做的事情是探测有没有一种pattern(图片样式，比如小鸟的鸟嘴)，我们知道这样的pattern是比整张图片小的，所以实际上neuron只需要看一张图的小部分，也就是说**每一个neuron要探测的部分更小对应更少的参数**。

![image-20201023205220144](C:\Users\liucheng\AppData\Roaming\Typora\typora-user-images\image-20201023205220144.png)

> The same patterns appear in different regions

同样的pattern可能出现在图片的不同部分，但是它们的形状相同，含义相同，应该是可以用同一个neuron，同样的参数，被同一个detector检测出来

![image-20201023205538134](C:\Users\liucheng\AppData\Roaming\Typora\typora-user-images\image-20201023205538134.png)

我们可以训练一个neuron，detector来做检测同一个pattern的工作，**它们可以共用一组参数，从而减少总参数的数量**。

> Subsampling the pixels will not change the object

我们可以对一张图片进行subsampling(二次抽样)，比如将图片的某些数量的行列拿掉，图片会变小，但是不影响人对图的理解。

![image-20201023210040721](C:\Users\liucheng\AppData\Roaming\Typora\typora-user-images\image-20201023210040721.png)

通过subsampling可以将图片变小，减少参数的数量。

## The whole CNN structure

整个CNN的架构，首先输入一张图片，它会通过Convolution的layer，然后做max pooling(最大池取样)然后重复两个步骤多次(**次数提前设定**)，做完Convolution、Max Pooling后进行Flatten,做完Flatten以后将output丢到fully connected network里面去，最终得到图像处理的结果。

![image-20201023210647952](C:\Users\liucheng\AppData\Roaming\Typora\typora-user-images\image-20201023210647952.png)

我们基于之前提到的对三个图像处理的观察，设计了CNN架构，第一个是探测pattern，只需要探测图片的一个小部分，第二个是同样的pattern会出现在不同的部分,第三个是可以对图片进行subsampling。

对于前二者，用Convolution的layer处理，第三个用max pooling处理。

![image-20201023211132713](C:\Users\liucheng\AppData\Roaming\Typora\typora-user-images\image-20201023211132713.png)

### Convolution

假设有一张6*6的图片作为输入，图片是黑白的，每个pixel中1代表黑色，在Convolution layer里面，有一堆filter,每一个filter实际上等同于fully connected layer里面的一个neuron。

#### property1

![image-20201023211533180](C:\Users\liucheng\AppData\Roaming\Typora\typora-user-images\image-20201023211533180.png) 

每个filter实际上是一个matrix，matrix里面每个element的值是network的参数，是根据训练集训练出来的。上图中每个filter是3*3的size，这代表它在探测一个3 * 3的pattern，**它在探测时候只看图片中3 * 3个pixel的区域**。

#### property2

filter从左上角开始，做slide window，每次向右边移动一定距离(stride),距离大小需要设计，每次filter停下的时候与对应的matrix做内积(相同位置相乘，并且累加求和)，这里假设stride=1，filter每次移动1格，当它碰到图片最右边，就从下一行最左边继续上诉操作，经过整个Convolution的处理，最终得到下图红色的4*4的matrix，观察可以知道，filter的作用就是检测有没有连续的从左上角到右下角的1，1，1，此时filter的卷积结果在左上角和左下角出现最大值，这就代表说filter探测的pattern出现在图片的左上角和右下角。

**我们探测到同一个pattern在图片的左上角和右下角，用了同一个filter，这与property2的考虑一样**。

#### Feature Map

在一个Convolution的layer里面，有很多filter，不同的filter有不同的参数，但是它们做卷积的过程是一样的，你用filter2和image做完Convolution以后，会得到另外一个蓝色的matrix,这个蓝色的matrix和之前的红色的matrix合起来，就叫做Feature Map(特征映射)，有多少个filter就有多少个映射后的image。

![feature map](img/feature map.jpg)

**CNN对不同scale的相同pattern处理存在一定的困难**，由于每个filter的size是一样的，这意味着如果你有同一个pattern，但是它的size不同，有大的鸟嘴也有小的鸟嘴，CNN不能自动处理这个问题。DeepMind曾经发过一篇paper，上面提到当你输入一张图片的时候它在CNN前面接另外一个network，这个network会输出一些scalar，告诉你把这个image里面的哪些位置做旋转、缩放，然后丢到CNN里面会得到比较好的performance。

#### Colorful image

对于彩色的image，彩色的image是由RGB组成的，你input的是好几个matrix叠在一起的一个立方体。

![colorful image](img/colorful image.jpg)

对应的filter也要变成一个立方体，如果用RGB三个颜色表示一个pixel,input会是3 * 6 * 6，你的filter会是3 * 3 * 3,filter的高是3，在做convolution的时候就是将filter的9个值和image的9个值进行内积，可以想象成filter的每一层都和分别image的三层做内积，得到的也是一个三层的output，每一个filter同时就考虑到了不同颜色代表的channel。

### Convolution vs Fully connected

#### filter是特殊的“neuron”

convolution实际上就是一个neural network，是一个fully connected的layer将一些weight拿掉(置0),下图绿色框框标识出来的filter map的output，实际上是hidden layer的neuron的output

![convolutionVSfullyconnected](/img/convolutionVSfullyconnected.jpg)

接下来我们解释这件事情：

如下图所示，我们做convolution的时候，将filter放在图片的左上角，做内积得到一个值3，这件事情等同于将image的matrix拉直变成右边一排用作input的vector，然后你有一个红色的neuron，这些input经过neuron之后得到的output是3

![explanation](/img/explanation.jpg)

#### 每个“neuron”只检测image的部分区域

这个neuron的output怎么来呢？实际上这个neuron实际上由filter转化而来，我们将filter放在image的左上角，filter对应考虑和image重合的9个pixel，假设你将这个6 * 6的36个pixel拉直的vector作为输入，那么这9个pixel就对应右边编号位1，2，3，7，8，9，13，14，15的pixel

这个filter和image matrix做内积得到3，就是input vector经过某个neuron得到output 3，就是说存在一个neuron，这个neuron带weight的连线只连接到上一段说的编号的9个pixel，weight的值对应filter matrix上的9个数值

作为对比，fully connected的neuron是必须连接36个input上的，但是现在我们只需要连接9个input，因为我们知道探测一个pattern只需要看3个input pixel，所以我们减少了参数的使用。

#### “neuron”之间共享参数

当我们把filter做stride = 1的移动时，我们通过filter和image matrix的内积得到另外一个output值-1，假设这个-1是另外一个neuron的output，这个neuron会连接到的input对应pixel 2，3，4，8，9，10，14，15，16

![share para](img/share para.jpg)

output为3和-1的两个neuron，它们分别检测在image的不同位置是否存在某个pattern，因为在fully connected layer它们做的事情不同应该有自己独立的weight

但是我们做convolution的时候，我们首先将每个neuron前面连接的weight减少了，然后强迫某些neuron(比如上一段的两个neuron)一定要共享一组weight，虽然两个neuron连接的pixel对象不同但是它们用的weight是一样的，等于filter里面的元素值，这就叫做weight share，又一次减少了参数的使用

#### 总结

我们可以这么思考，有一些特殊的neuron，它们只连接9条带着weight的线(9=3 * 3对应filter matrix的元素个数，weight是元素值，上图圆圈的颜色和连线的颜色一一对应)

当filter在image matrix上移动做convolution的时候，每次移动都是检测这个地方有没有某个pattern，对于fully connected layer是要对整个image进行检测的，所以每次检测image不同地方是否有pattern是不同的事情，这些neuron必须连接到整张image的所有pixel上，并且不同的neuron连线上的weight是相互独立的。

**对于Convolution layer来说，首先它是对image一部分做检测，因此它的neuron只需要连接image部分的pixel，对应连线所需要的weight参数就会减少，其次由于是用同一个filter检测不同位置的pattern，所以对应Convolution layer是同一件事情，不同的neuron连接的pixel对象不同，但是在“做同一件事情”的前提下，也就是用同一个filter的前提下，这些neuron所用的weight参数是相同的，通过weight share的方式减少了network所需要用到的weight参数。CNN本质是减少参数的过程。**

#### 补充

CNN怎么搭建和train呢？

首先，第一件事情就是这都是用toolkit做的，所以你大概不会自己去写；如果你要自己写的话，它其实就是跟原来的Backpropagation用一模一样的做法，只是有一些weight就永远是0，你就不去train它，它就永远是0

然后，怎么让某些neuron的weight值永远都是一样呢？你就用一般的Backpropagation的方法，对每个weight都去算出gradient，再把本来要tight在一起、要share weight的那些weight的gradient平均，然后，让他们update同样值就ok了